{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical DQN from Scratch\n",
    "\n",
    "The authors of categorical DQN argue that there is information value in learning not the expected return, but the distribution of returns. They state the key insight is that the distribution returns satisfy a variant of the Bellman equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the notebook is located inside `subfolder` and\n",
    "# we want to import a module in `parent_folder`\n",
    "\n",
    "# Get the absolute path to the 'subfolder'.\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Calculate the path to the 'parent_folder' by going one level up.\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "\n",
    "# Add the 'parent_folder' to sys.path if it is not already there.\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from numpy import ndarray\n",
    "from IPython.display import clear_output\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from agents import MlpDQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Class representing a Categorical Neural Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, out_dim: int, atom_size: int, support: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Instantiates a CategoricalNetwork.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.support = support\n",
    "        self.out_dim = out_dim\n",
    "        self.in_dim = in_dim\n",
    "        self.atom_size = atom_size\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim * atom_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the neural network.\n",
    "        \"\"\"\n",
    "        dist = self.dist(x)\n",
    "\n",
    "        return torch.sum(dist * self.support, dim=2)\n",
    "    \n",
    "    def dist(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Gets a distribution over atoms.\n",
    "        \"\"\"\n",
    "        q_atoms = self.layers(x).view(-1, self.out_dim, self.atom_size)\n",
    "        dist = F.softmax(q_atoms, dim=-1)\n",
    "        # To avoid NaNs\n",
    "        dist = dist.clamp(min=1e-3)\n",
    "\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalDQNAgent(MlpDQNAgent):\n",
    "\n",
    "    def __init__(\n",
    "        self,        \n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        epsilon_decay: float = 1e-4,\n",
    "        seed: int = 42,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.1,\n",
    "        gamma: float = 0.99,\n",
    "        # Categorical DQN Parameters\n",
    "        v_min: float = 0.0,\n",
    "        v_max: float = 200.0,\n",
    "        atom_size: int = 51,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            env,\n",
    "            memory_size,\n",
    "            batch_size,\n",
    "            target_update,\n",
    "            epsilon_decay,\n",
    "            seed,\n",
    "            max_epsilon,\n",
    "            min_epsilon,\n",
    "            gamma,\n",
    "        )\n",
    "\n",
    "        # Categorical DQN Parameters\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.atom_size = atom_size\n",
    "        self.support = torch.linspace(\n",
    "            v_min, v_max, atom_size\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Categorical Networks\n",
    "        self.dqn = CategoricalNetwork(\n",
    "            self.obs_dim, self.action_dim, atom_size, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target = CategoricalNetwork(\n",
    "            self.obs_dim, self.action_dim, atom_size, self.support\n",
    "        ).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        # Optimiser\n",
    "        self.optimiser = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, ndarray]) -> torch.Tensor:\n",
    "        device = self.device\n",
    "\n",
    "        # Shape = (batch_size, obs dim 1, obs dim 2, ...)\n",
    "        # This flattens the observation dimensions of `state` and `next_state`.\n",
    "        state = torch.FloatTensor(samples[\"obs\"].reshape(self.batch_size, -1)).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"].reshape(self.batch_size, -1)).to(device)\n",
    "\n",
    "        # Reshapes each 1-dimesional array into a 2-dimensional array with one column.\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "\n",
    "        # Categorical DQN algorithm.\n",
    "        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action = self.dqn_target(next_state).argmax(1)\n",
    "            next_dist = self.dqn_target.dist(next_state)\n",
    "            next_dist = next_dist[range(self.batch_size), next_action]\n",
    "\n",
    "            t_z = reward + (1 - done) * self.gamma * self.support\n",
    "            t_z = t_z.clamp(min=self.v_min, max=self.v_max)\n",
    "            b = (t_z - self.v_min) / delta_z\n",
    "            l = b.floor().long()\n",
    "            u = b.ceil().long()\n",
    "\n",
    "            offset = (\n",
    "                torch.linspace(\n",
    "                    0, (self.batch_size - 1) * self.atom_size, self.batch_size\n",
    "                ).long()\n",
    "                .unsqueeze(1)\n",
    "                .expand(self.batch_size, self.atom_size)\n",
    "                .to(self.device)\n",
    "            )\n",
    "\n",
    "            proj_dist = torch.zeros(next_dist.size(), device=self.device)\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n",
    "            )\n",
    "            proj_dist.view(-1).index_add_(\n",
    "                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n",
    "            )\n",
    "\n",
    "        dist = self.dqn.dist(state)\n",
    "        log_p = torch.log(dist[range(self.batch_size), action])\n",
    "\n",
    "        loss = -(proj_dist * log_p).sum(1).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 777\n",
    "num_frames = 100_000\n",
    "memory_size = 4_000\n",
    "gamma = 0.8\n",
    "batch_size = 64\n",
    "target_update = 50\n",
    "epsilon_decay = 1 / (num_frames // 10)\n",
    "\n",
    "agent = CategoricalDQNAgent(\n",
    "    env,\n",
    "    memory_size,\n",
    "    batch_size,\n",
    "    target_update,\n",
    "    epsilon_decay,\n",
    "    seed,\n",
    "    gamma=gamma,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_lens, ep_rews = agent.test(100, time_interval=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
