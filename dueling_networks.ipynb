{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling Networks from Scratch\n",
    "\n",
    "The dueling architecture explicitly separates the representation of state values from (state-dependent) action advantage values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from buffers import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"\n",
    "        Instantiates a Duelling Neural Network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Common feature layer.\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Advantage layer\n",
    "        self.advantage_layer = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim),\n",
    "        )\n",
    "\n",
    "        # Value layer\n",
    "        self.value_layer = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the neural network.\n",
    "        \"\"\"\n",
    "        feature = self.feature_layer(x)\n",
    "\n",
    "        s_value = self.value_layer(feature)\n",
    "        advantage = self.advantage_layer(feature)\n",
    "\n",
    "        q_value = s_value + advantage - advantage.mean(dim=-1, keepdim=True)\n",
    "\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNAgent:\n",
    "    \"\"\"\n",
    "    Class representing a DQN agent with Dueling Networks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        epsilon_decay: float,\n",
    "        seed: int,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        obs_dim = np.prod(env.observation_space.shape)\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = max_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.seed = seed\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "\n",
    "        # Networks: DQN behaviour network, DQN target network\n",
    "        self.dqn = DuelingNetwork(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target = DuelingNetwork(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        # Optimiser\n",
    "        self.optimiser = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # Transition to store in memory\n",
    "        self.transition = list()\n",
    "\n",
    "        # Mode: train/test.\n",
    "        self.is_test = False\n",
    "\n",
    "    def predict(self, state: np.ndarray, determinstic: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Selects an action from the input state using a (potentially) epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        return self.select_action(state, determinstic)\n",
    "\n",
    "    def select_action(self, state: np.ndarray, determinstic: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Selects an action from the input state using an epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if not determinstic and np.random.random() < self.epsilon:\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            selected_action = self.dqn(\n",
    "                torch.FloatTensor(state).to(self.device)\n",
    "            ).argmax()\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "\n",
    "        return selected_action\n",
    "    \n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"\n",
    "        Takes an action and returns the response of the env.\n",
    "        \"\"\"\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state = next_state.flatten()\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Updates the model by gradient descent.\n",
    "        \"\"\"\n",
    "        samples = self.memory.sample_batch()\n",
    "\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "        \n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Dueling DQN: We clip the gradients to have their norm less than or equal to 10.\n",
    "        clip_grad_norm_(self.dqn.parameters(), 10.0)\n",
    "\n",
    "        self.optimiser.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"\n",
    "        Trains the agent.\n",
    "        \"\"\"\n",
    "        self.is_test = False\n",
    "\n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        state = state.flatten() # Necessary for the highway environment\n",
    "\n",
    "        update_count = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(1, num_frames + 1):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                # The episode has ended.\n",
    "                state, _ = self.env.reset(seed=self.seed)\n",
    "                state = state.flatten()\n",
    "                \n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                # Training is ready once the replay buffer contains enough transition samples.\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_count += 1\n",
    "\n",
    "                # Linearly decrease epsilon\n",
    "                self.epsilon = max(\n",
    "                    self.min_epsilon,\n",
    "                    self.epsilon - (\n",
    "                        self.max_epsilon - self.min_epsilon\n",
    "                    ) * self.epsilon_decay\n",
    "                )\n",
    "                epsilons.append(self.epsilon)\n",
    "\n",
    "                # If a hard update of the target network is needed.\n",
    "                if update_count % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "\n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                self._plot(frame_idx, scores, losses, epsilons)\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, video_folder: str) -> None:\n",
    "        \"\"\"\n",
    "        Tests the agent.\n",
    "        \"\"\"\n",
    "        self.is_test =True\n",
    "        \n",
    "        naive_env = self.env\n",
    "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)\n",
    "\n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        state = state.flatten()\n",
    "\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = self.predict(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "            next_state = next_state.flatten()\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        # Reset\n",
    "        self.env = naive_env\n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes and returns the DQN loss.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "\n",
    "        # G_t = r + gamma * v(s_{t+1}) if state != terminal\n",
    "        #     = r                      otherwise\n",
    "        curr_q_value = self.dqn(state).gather(1, action)\n",
    "        next_q_value = self.dqn_target(next_state).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - done\n",
    "        target = (reward + self.gamma * next_q_value * mask).to(device)\n",
    "\n",
    "        # Calculate DQN loss\n",
    "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"\n",
    "        Performs a hard update of the target network: target <- behavioural.\n",
    "        \"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _plot(\n",
    "        self, \n",
    "        frame_idx: int, \n",
    "        scores: List[float], \n",
    "        losses: List[float], \n",
    "        epsilons: List[float],\n",
    "    ):\n",
    "        \"\"\"Plots the training progress.\"\"\"\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.subplot(133)\n",
    "        plt.title('epsilons')\n",
    "        plt.plot(epsilons)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"highway-fast-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "seed = 777\n",
    "\n",
    "def seed_torch(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)\n",
    "\n",
    "num_frames = 10_000\n",
    "memory_size = 15_000\n",
    "gamma = 0.8\n",
    "batch_size = 64\n",
    "target_update = 50\n",
    "epsilon_decay = 1 / 4_000\n",
    "\n",
    "agent = DuelingDQNAgent(\n",
    "    env,\n",
    "    memory_size,\n",
    "    batch_size,\n",
    "    target_update,\n",
    "    epsilon_decay,\n",
    "    seed,\n",
    "    gamma = gamma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    done = truncated = False\n",
    "    obs, info = env.reset()\n",
    "    obs = obs.flatten()\n",
    "\n",
    "    while not (done or truncated):\n",
    "        action = agent.predict(obs, True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        obs = obs.flatten()\n",
    "\n",
    "        clear_output(True)\n",
    "        plt.imshow(env.render())\n",
    "        plt.show()\n",
    "        time.sleep(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
